{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import pickle \n",
    "#import mglearn\n",
    "import time\n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = [\"He is ::having a great Time, at the park time?\",\n",
    "       \"She, unlike most women, is a big player on the park's grass.\",\n",
    "       \"she can't be going\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature:\n",
      "['big', 'going', 'grass', 'great', 'having', 'park', 'player', 'time', 'unlike', 'women']\n",
      "\n",
      "Every 3rd feature:\n",
      "['big', 'great', 'player', 'women']\n"
     ]
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "# Transforms the data into a bag of words\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "# Print the first 10 features of the count_vec\n",
    "print(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\n",
    "print(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names()[::3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10\n",
      "Vocabulary content:\n",
      " {'having': 4, 'great': 3, 'time': 7, 'park': 5, 'unlike': 8, 'women': 9, 'big': 0, 'player': 6, 'grass': 2, 'going': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'big player', 'going', 'grass', 'great', 'great time', 'having', 'having great', 'park', 'park grass', 'park time', 'player', 'player park', 'time', 'time park', 'unlike', 'unlike women', 'women', 'women big']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'big player', 'big player park', 'going', 'grass', 'great', 'great time', 'great time park', 'having', 'having great', 'having great time', 'park', 'park grass', 'park time', 'player', 'player park', 'player park grass', 'time', 'time park', 'time park time', 'unlike', 'unlike women', 'unlike women big', 'women', 'women big', 'women big player']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['park']\n",
      "\n",
      "Only 'park' becomes the vocabulary of the document term matrix (dtm) because it appears in 2 out of 3 documents, meaning 0.66% of the time.      \n",
      "The rest of the words such as 'big' appear only in 1 out of 3 documents, meaning 0.33%. which is why they don't appear\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=0.6, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())\n",
    "print(\"\\nOnly 'park' becomes the vocabulary of the document term matrix (dtm) because it appears in 2 out of 3 documents, \\\n",
    "meaning 0.66% of the time.\\\n",
    "      \\nThe rest of the words such as 'big' appear only in 1 out of 3 documents, meaning 0.33%. which is why they don't appear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'going', 'grass', 'great', 'having', 'player', 'time', 'unlike', 'women']\n",
      "\n",
      "Only 'park' is ignored because it appears in 2 out of 3 documents, meaning 0.66% of the time.\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=0.50, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())\n",
    "print(\"\\nOnly 'park' is ignored because it appears in 2 out of 3 documents, meaning 0.66% of the time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'going', 'park', 'time']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=4)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer -- Brief Tutorial\n",
    "\n",
    "The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. (https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/feature_extraction/text.py#L1365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text:  ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\n"
     ]
    }
   ],
   "source": [
    "txt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\n",
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted = tf.fit(txt1)\n",
    "txt_transformed = txt_fitted.transform(txt1)\n",
    "print (\"The text: \", txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'his': 0, 'not': 1, 'perfect': 2, 'sang': 3, 'she': 4, 'smile': 5, 'was': 6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his': 1.4054651081081644, 'not': 1.0, 'perfect': 1.4054651081081644, 'sang': 2.09861228866811, 'she': 2.09861228866811, 'smile': 1.4054651081081644, 'was': 1.4054651081081644}\n",
      "\n",
      "We see that the tokens 'sang','she' have the most idf weight because they are the only tokens that appear in one document only.\n",
      "\n",
      "The token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\n"
     ]
    }
   ],
   "source": [
    "idf = tf.idf_\n",
    "print(dict(zip(txt_fitted.get_feature_names(), idf)))\n",
    "print(\"\\nWe see that the tokens 'sang','she' have the most idf weight because \\\n",
    "they are the only tokens that appear in one document only.\")\n",
    "print(\"\\nThe token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = dict(zip(txt_fitted.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'his': 1.4054651081081644,\n",
       " 'not': 1.0,\n",
       " 'perfect': 1.4054651081081644,\n",
       " 'sang': 2.09861228866811,\n",
       " 'she': 2.09861228866811,\n",
       " 'smile': 1.4054651081081644,\n",
       " 'was': 1.4054651081081644}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFKCAYAAACzX0NnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\n",
    "token_weight.columns=('token','weight')\n",
    "token_weight = token_weight.sort_values(by='weight', ascending=False)\n",
    "token_weight \n",
    "\n",
    "sns.barplot(x='token', y='weight', data=token_weight)            \n",
    "plt.title(\"Inverse Document Frequency(idf) per token\")\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['not' 'his' 'perfect']\n",
      "\n",
      "Features with highest idf:\n",
      "['was' 'sang' 'she']\n"
     ]
    }
   ],
   "source": [
    "# get feature names\n",
    "feature_names = np.array(tf.get_feature_names())\n",
    "sorted_by_idf = np.argsort(tf.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:3]]))\n",
    "print(\"\\nFeatures with highest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[-3:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1 its weight is 0 because it does not appear there.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.40546511,  1.        ,  1.40546511,  0.        ,  0.        ,\n",
       "         1.40546511,  1.40546511],\n",
       "       [ 1.40546511,  4.        ,  1.40546511,  0.        ,  0.        ,\n",
       "         1.40546511,  1.40546511],\n",
       "       [ 0.        ,  1.        ,  0.        ,  2.09861229,  2.09861229,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1\\\n",
    " its weight is 0 because it does not appear there.\")\n",
    "txt_transformed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF - Maximum token value throughout the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "holdout = pd.read_csv('test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 6.052999973297119\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "# remove '\\\\n'\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove '\\\\n'\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "    \n",
    "# remove any text starting with User... \n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "holdout['comment_text'] = holdout['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train['comment_text']\n",
    "y = train.iloc[:, 2:8]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656,)\n",
      "(127656, 6)\n",
      "(31915,)\n",
      "(31915, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 37.07191276550293\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "# Instantiate the vectorizer\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=30000)\n",
    "\n",
    "# fit and transform on it the training features\n",
    "word_vectorizer.fit(X_train)\n",
    "X_train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "test_features = word_vectorizer.transform(X_test)\n",
    "\n",
    "# transform the holdout text for submission at the end\n",
    "holdout_text = holdout['comment_text']\n",
    "holdout_word_features = word_vectorizer.transform(holdout_text)\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Log_loss score for class toxic is -0.11565060036541504\n",
      "CV Accuracy score for class toxic is 0.9589835178269303\n",
      "CV ROC_AUC score 0.9655633052947897\n",
      "\n",
      "[[28571   356]\n",
      " [  965  2023]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98     28927\n",
      "          1       0.85      0.68      0.75      2988\n",
      "\n",
      "avg / total       0.96      0.96      0.96     31915\n",
      "\n",
      "CV Log_loss score for class severe_toxic is -0.02870223173759266\n",
      "CV Accuracy score for class severe_toxic is 0.9901923827175312\n",
      "CV ROC_AUC score 0.9819670455594212\n",
      "\n",
      "[[31533    79]\n",
      " [  213    90]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     31612\n",
      "          1       0.53      0.30      0.38       303\n",
      "\n",
      "avg / total       0.99      0.99      0.99     31915\n",
      "\n",
      "CV Log_loss score for class obscene is -0.06353778626883393\n",
      "CV Accuracy score for class obscene is 0.9784577368944316\n",
      "CV ROC_AUC score 0.9825812057362143\n",
      "\n",
      "[[30131   160]\n",
      " [  470  1154]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     30291\n",
      "          1       0.88      0.71      0.79      1624\n",
      "\n",
      "avg / total       0.98      0.98      0.98     31915\n",
      "\n",
      "CV Log_loss score for class threat is -0.009942435696162542\n",
      "CV Accuracy score for class threat is 0.9972739219476136\n",
      "CV ROC_AUC score 0.977970323819499\n",
      "\n",
      "[[31811    14]\n",
      " [   66    24]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     31825\n",
      "          1       0.63      0.27      0.38        90\n",
      "\n",
      "avg / total       1.00      1.00      1.00     31915\n",
      "\n",
      "CV Log_loss score for class insult is -0.08326876489595732\n",
      "CV Accuracy score for class insult is 0.9704596755459962\n",
      "CV ROC_AUC score 0.971019762833052\n",
      "\n",
      "[[30153   259]\n",
      " [  653   850]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     30412\n",
      "          1       0.77      0.57      0.65      1503\n",
      "\n",
      "avg / total       0.97      0.97      0.97     31915\n",
      "\n",
      "CV Log_loss score for class identity_hate is -0.025804586888987187\n",
      "CV Accuracy score for class identity_hate is 0.9920646143217648\n",
      "CV ROC_AUC score 0.9725883716583562\n",
      "\n",
      "[[31590    48]\n",
      " [  204    73]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     31638\n",
      "          1       0.60      0.26      0.37       277\n",
      "\n",
      "avg / total       0.99      0.99      0.99     31915\n",
      "\n",
      "Total average CV Log_loss score is -0.05448440097549145\n",
      "Total average CV ROC_AUC score is 0.9752816691502222\n"
     ]
    }
   ],
   "source": [
    "class_names = ['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "losses = []\n",
    "auc = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    #call the labels one column at a time so we can run the classifier on them\n",
    "    train_target = y_train[class_name]\n",
    "    test_target = y_test[class_name]\n",
    "    classifier = LogisticRegression(solver='sag', C=10)\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='neg_log_loss'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='accuracy'))\n",
    "    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(X_train_word_features, train_target)\n",
    "    y_pred = classifier.predict(test_features)\n",
    "    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n",
    "    auc.append(auc_score)\n",
    "    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n",
    "    \n",
    "    print(confusion_matrix(test_target, y_pred))\n",
    "    print(classification_report(test_target, y_pred))\n",
    "\n",
    "print('Total average CV Log_loss score is {}'.format(np.mean(losses)))\n",
    "print('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize, Classify (with parameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train['comment_text']\n",
    "y = train.iloc[:, 2:8]  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 329.3189539909363\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(\n",
    "                                    stop_words='english',\n",
    "                                    strip_accents='unicode',\n",
    "                                    token_pattern=r'\\w{1,}', #accept tokens that have 1 or more characters\n",
    "                                    analyzer='word',\n",
    "                                    ngram_range=(1, 1),\n",
    "                                    min_df=5),\n",
    "                     OneVsRestClassifier(LogisticRegression()))\n",
    "param_grid = {'tfidfvectorizer__max_features': [10000, 30000],\n",
    "              'onevsrestclassifier__estimator__solver': ['liblinear', 'sag'],\n",
    "             } \n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, scoring='roc_auc')\n",
    "\n",
    "grid3 = grid.fit(X_train, y_train)\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "          n_jobs=1)\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=30000, min_df=5,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(grid3.best_estimator_.named_steps['onevsrestclassifier'])\n",
    "print(grid3.best_estimator_.named_steps['tfidfvectorizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'onevsrestclassifier__estimator__solver': 'sag',\n",
       " 'tfidfvectorizer__max_features': 30000}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97782430112632579"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y_test = grid3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic Confusion Matrixs: \n",
      "[[28766   161]\n",
      " [ 1235  1753]]\n",
      "\n",
      "Severe Toxic: \n",
      "[[31558    54]\n",
      " [  234    69]]\n",
      "\n",
      "Obscene: \n",
      "[[30209    82]\n",
      " [  598  1026]]\n",
      "\n",
      "Threat: \n",
      "[[31815    10]\n",
      " [   76    14]]\n",
      "\n",
      "Insult: \n",
      "[[30235   177]\n",
      " [  755   748]]\n",
      "\n",
      "Identity Hate: \n",
      "[[31617    21]\n",
      " [  233    44]]\n",
      "\n",
      "Toxic Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98     28927\n",
      "          1       0.92      0.59      0.72      2988\n",
      "\n",
      "avg / total       0.95      0.96      0.95     31915\n",
      "\n",
      "\n",
      "Severe Toxic: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     31612\n",
      "          1       0.56      0.23      0.32       303\n",
      "\n",
      "avg / total       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "Obscene: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     30291\n",
      "          1       0.93      0.63      0.75      1624\n",
      "\n",
      "avg / total       0.98      0.98      0.98     31915\n",
      "\n",
      "\n",
      "Threat: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     31825\n",
      "          1       0.58      0.16      0.25        90\n",
      "\n",
      "avg / total       1.00      1.00      1.00     31915\n",
      "\n",
      "\n",
      "Insult: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98     30412\n",
      "          1       0.81      0.50      0.62      1503\n",
      "\n",
      "avg / total       0.97      0.97      0.97     31915\n",
      "\n",
      "\n",
      "Identity Hate: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     31638\n",
      "          1       0.68      0.16      0.26       277\n",
      "\n",
      "avg / total       0.99      0.99      0.99     31915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxic Confusion Matrixs: \\n{}\".format(confusion_matrix(y_test['toxic'], predicted_y_test[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(confusion_matrix(y_test['severe_toxic'], predicted_y_test[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(confusion_matrix(y_test['obscene'], predicted_y_test[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(confusion_matrix(y_test['threat'], predicted_y_test[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(confusion_matrix(y_test['insult'], predicted_y_test[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(confusion_matrix(y_test['identity_hate'], predicted_y_test[:,5])))\n",
    "\n",
    "print(\"\\nToxic Classification report: \\n{}\".format(classification_report(y_test['toxic'], predicted_y_test[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(classification_report(y_test['severe_toxic'], predicted_y_test[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(classification_report(y_test['obscene'], predicted_y_test[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(classification_report(y_test['threat'], predicted_y_test[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(classification_report(y_test['insult'], predicted_y_test[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(classification_report(y_test['identity_hate'], predicted_y_test[:,5])))\n",
    "#Toxic Confusion Matrixs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['overstating' 'tusc' 'leona' 'ashish' 'tvs' 'divorcing' 'chandler' 'spoon'\n",
      " 'ashoka' 'ashraf' 'chandigarh' 'sporadically' 'jurors' 'queues' 'diwan'\n",
      " 'grabs' 'sportsman' 'oppressing' 'warms' 'aslo']\n",
      "\n",
      "Features with highest tfidf: \n",
      "['russian' 'source' 'yes' 's' 'paranoia' 'called' 'template' 'natural'\n",
      " 'fish' 'soon' 'understand' 'fine' 'canon' 'nazi' 'inserted' 'solved'\n",
      " 'information' 'career' 'filter' 'pwned']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = grid3.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# transform the training dataset:\n",
    "X_test_set = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# find maximum value for each of the features over dataset:\n",
    "max_value = X_test_set.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "# get feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"\\nFeatures with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['s' 't' 'article' 'page' 'talk' 'wikipedia' 'just' 'like' 'don' 'think'\n",
      " 'know' 'm' 'edit' 'people' 'time' 'did' 'thanks' 'articles' 'make' 'use'\n",
      " 'good' 'does' 've' 'want' 'way' 'need' 'thank' 'information' 'editing'\n",
      " 'user' 'say' 'new' 'really' 'pages' 'sources' 'help' 'edits' 'look'\n",
      " 'section' 'add' 'source' 'wp' 'used' 'right' 'work' 'point' 'discussion'\n",
      " 'read' 'll' 'deleted' 'fact' 'utc' 'going' 'blocked' 'stop' 'said' 'd'\n",
      " 'added' 'sure' 'removed' 'link' 'let' 'better' 'doesn' 'reason' 'feel'\n",
      " 'list' 'place' 'actually' 'note' 'content' 'case' 'using' 'history' 'hi'\n",
      " 'believe' 'deletion' 'editors' 'free' '2' 'comment' 'things' 'person'\n",
      " 'question' 'personal' 'thing' '1' 'didn' 'ask' 'comments' 'vandalism'\n",
      " 'best' 'remove' 'hope' 'wrong' 'problem' 'little' 'trying' 'change' 'wiki']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdout_comments = holdout['comment_text']\n",
    "# holdoutComments are automatically transformed throguh the grid3 pipeline before prodicting probabilities\n",
    "twod = grid3.predict_proba(holdout_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdout_predictions = {}\n",
    "holdout_predictions = {'id': holdout['id']}  \n",
    "\n",
    "holdout_predictions['toxic']=twod[:,0]\n",
    "holdout_predictions['severe_toxic']=twod[:,1]\n",
    "holdout_predictions['obscene']=twod[:,2]\n",
    "holdout_predictions['threat']=twod[:,3]\n",
    "holdout_predictions['insult']=twod[:,4]\n",
    "holdout_predictions['identity_hate']=twod[:,5]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(holdout_predictions)\n",
    "submission = submission[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']] #rearrange columns\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Adding features to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate lenght of characters in each comment\n",
    "train['len_character'] = train['comment_text'].apply(lambda x: len(re.findall(r\"[\\w]\", str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion #unites all arrays into one array\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train[['comment_text','len_character']] #these will be our features\n",
    "y = train.iloc[:, 2:8]  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656,)\n",
      "(127656, 1)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda a: a[['len_character']], validate=False)\n",
    "get_text_data = FunctionTransformer(lambda a: a['comment_text'], validate=False)\n",
    "\n",
    "print(get_text_data.fit_transform(X_train).shape)\n",
    "print(get_numeric_data.fit_transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(                      #unites both text and numeric arrays into one array\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data)\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', TfidfVectorizer(\n",
    "                                                    stop_words='english',\n",
    "                                                    strip_accents='unicode',\n",
    "                                                    token_pattern=r'\\w{2,}',\n",
    "                                                    analyzer='word',\n",
    "                                                    ngram_range=(1, 1),\n",
    "                                                    min_df=5))\n",
    "                ]))\n",
    "             ]\n",
    "        )), #right here is where we would put interaction terms preprocessing such as PolynomialFeatures\n",
    "            #(right here is where we would put a scaler if we needed one)\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression())) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'union__text_features__vectorizer__max_features': [10000, 30000],\n",
    "              'clf__estimator__C': [0.1, 1]\n",
    "             } \n",
    "grid = GridSearchCV(pl, param_grid, cv=3, scoring='roc_auc')\n",
    "\n",
    "grid4 = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976986853037\n",
      "{'clf__estimator__C': 1, 'union__text_features__vectorizer__max_features': 30000}\n",
      "Pipeline(memory=None,\n",
      "     steps=[('union', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('numeric_features', Pipeline(memory=None,\n",
      "     steps=[('selector', FunctionTransformer(accept_sparse=False,\n",
      "          func=<function <lambda> at 0x000001B9225000D0>, inv_kw_args=None,\n",
      "          inverse_func=None, kw_args=None, pass_y=...=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "          n_jobs=1))])\n"
     ]
    }
   ],
   "source": [
    "print(grid4.best_score_)\n",
    "print(grid4.best_params_)\n",
    "print(grid4.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic Confusion Matrixs: \n",
      "[[28760   167]\n",
      " [ 1244  1744]]\n",
      "\n",
      "Severe Toxic: \n",
      "[[31556    56]\n",
      " [  229    74]]\n",
      "\n",
      "Obscene: \n",
      "[[30228    63]\n",
      " [  717   907]]\n",
      "\n",
      "Threat: \n",
      "[[31814    11]\n",
      " [   75    15]]\n",
      "\n",
      "Insult: \n",
      "[[30240   172]\n",
      " [  768   735]]\n",
      "\n",
      "Identity Hate: \n",
      "[[31616    22]\n",
      " [  230    47]]\n",
      "\n",
      "Toxic Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98     28927\n",
      "          1       0.91      0.58      0.71      2988\n",
      "\n",
      "avg / total       0.95      0.96      0.95     31915\n",
      "\n",
      "\n",
      "Severe Toxic: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     31612\n",
      "          1       0.57      0.24      0.34       303\n",
      "\n",
      "avg / total       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "Obscene: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     30291\n",
      "          1       0.94      0.56      0.70      1624\n",
      "\n",
      "avg / total       0.97      0.98      0.97     31915\n",
      "\n",
      "\n",
      "Threat: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     31825\n",
      "          1       0.58      0.17      0.26        90\n",
      "\n",
      "avg / total       1.00      1.00      1.00     31915\n",
      "\n",
      "\n",
      "Insult: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98     30412\n",
      "          1       0.81      0.49      0.61      1503\n",
      "\n",
      "avg / total       0.97      0.97      0.97     31915\n",
      "\n",
      "\n",
      "Identity Hate: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     31638\n",
      "          1       0.68      0.17      0.27       277\n",
      "\n",
      "avg / total       0.99      0.99      0.99     31915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y_test = grid4.predict(X_test)\n",
    "\n",
    "print(\"Toxic Confusion Matrixs: \\n{}\".format(confusion_matrix(y_test['toxic'], pred_y_test[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(confusion_matrix(y_test['severe_toxic'], pred_y_test[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(confusion_matrix(y_test['obscene'], pred_y_test[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(confusion_matrix(y_test['threat'], pred_y_test[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(confusion_matrix(y_test['insult'], pred_y_test[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(confusion_matrix(y_test['identity_hate'], pred_y_test[:,5])))\n",
    "\n",
    "print(\"\\nToxic Classification report: \\n{}\".format(classification_report(y_test['toxic'], pred_y_test[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(classification_report(y_test['severe_toxic'], pred_y_test[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(classification_report(y_test['obscene'], pred_y_test[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(classification_report(y_test['threat'], pred_y_test[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(classification_report(y_test['insult'], pred_y_test[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(classification_report(y_test['identity_hate'], pred_y_test[:,5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
